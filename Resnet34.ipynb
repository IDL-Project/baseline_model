{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TfvYnyraP2B"
   },
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htZPsQFv9qus",
    "outputId": "b7b8ed9b-4856-41a5-878a-0c7719e33996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 26 17:51:25 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1PFM5KmaXId",
    "outputId": "b3420892-2333-47d8-eb2e-98b350ff50c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8O8J7svW4BU"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vgmp8z-FW4BV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pdb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdLZDLLslqec"
   },
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fI5M46mlu76"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 100\n",
    "in_features = 3 # RGB channels\n",
    "\n",
    "learningRate = 0.1\n",
    "weightDecay = 5e-5\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "feat_dim = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91JRT1mdW4Bp"
   },
   "source": [
    "## Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Y2muYrW4Bq"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.expansion = 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Te7OEcCW4Bq"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers, block, image_channels, num_classes,feat_dim = 1024):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.expansion = 1\n",
    "        layers = [3, 4, 6, 3]\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNetLayers\n",
    "        self.layer1 = self.make_layers(num_layers, block, layers[0], intermediate_channels=64, stride=1)\n",
    "        self.layer2 = self.make_layers(num_layers, block, layers[1], intermediate_channels=128, stride=2)\n",
    "        self.layer3 = self.make_layers(num_layers, block, layers[2], intermediate_channels=256, stride=2)\n",
    "        self.layer4 = self.make_layers(num_layers, block, layers[3], intermediate_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "        self.linear = nn.Linear(512 * self.expansion, feat_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear_output = nn.Linear(512 * self.expansion,num_classes)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        embedding = x\n",
    "        embedding_out = self.relu(self.linear(embedding))\n",
    "        output = self.linear_output(embedding)\n",
    "        if return_embedding:\n",
    "            return embedding_out,output\n",
    "        else:\n",
    "            return output  \n",
    "\n",
    "    def make_layers(self, num_layers, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        layers = []\n",
    "\n",
    "        identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, intermediate_channels*self.expansion, kernel_size=1, stride=stride),\n",
    "                                            nn.BatchNorm2d(intermediate_channels*self.expansion))\n",
    "        layers.append(block(num_layers, self.in_channels, intermediate_channels, identity_downsample, stride))\n",
    "        self.in_channels = intermediate_channels * self.expansion # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(num_layers, self.in_channels, intermediate_channels)) # 256 -> 64, 64*4 (256) again\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSF0iV5JhVh5"
   },
   "source": [
    "## Load in Data (with augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDNRAVftW4Br"
   },
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(20),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "val_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='train_data/', \n",
    "                                                 transform=transforms)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True, num_workers=4)\n",
    "# deal with class_to_idx problem\n",
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageFolder(root='val_data/', \n",
    "                                               transform=val_transforms)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, \n",
    "                                             shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcisVhbGW4Bs"
   },
   "source": [
    "### Extra setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1yCr3JOW4Bt"
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(34, Block, in_features, num_classes)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate,  momentum=0.9, weight_decay=weightDecay, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jZ0kA0fZlx-"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, epoch_num):\n",
    "    print(f\"Epoch {epoch_num}...\")\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad() \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target.long())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "        loss.backward() # Calculate gradients\n",
    "        optimizer.step() # Apply gradient descent step\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    print('Train time (min):', (end_time - start_time)/60)\n",
    "    print()\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD1NTiROa8ii"
   },
   "outputs": [],
   "source": [
    "# Validation/ Evaluation Function\n",
    "def val_model(model, val_loader, criterion, scheduler):\n",
    "    print('Validating...')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step(running_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "        running_loss /= len(val_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Loss: ', running_loss)\n",
    "        print('Validation Accuracy: ', acc, '%')\n",
    "        print('Validation time (min):', (end_time - start_time)/60)\n",
    "        print()\n",
    "        return running_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2x9g91fbbQC"
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "trialNum = 2\n",
    "best_epoch = 0\n",
    "winner = \"\"\n",
    "\n",
    "current_best = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS): \n",
    "    train_loss, train_acc = train_epoch(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    val_loss, val_acc = val_model(model, val_dataloader, criterion, scheduler=scheduler)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    if val_acc > current_best:\n",
    "      # Get old model to be deleted\n",
    "      old_model = f\"Trial{trialNum}-epoch{best_epoch}.pth\"\n",
    "\n",
    "      # Save new best model\n",
    "      current_best = val_acc\n",
    "      best_epoch = epoch\n",
    "      winner = f\"Trial{trialNum}-epoch{epoch}\"\n",
    "      filename = save_location_raw+winner+\".pth\"\n",
    "      torch.save(model.state_dict(), filename)\n",
    "\n",
    "      # Delete old model if exists\n",
    "      if os.path.exists(old_model):\n",
    "        os.remove(old_model)\n",
    "\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZlWltAgcLty"
   },
   "outputs": [],
   "source": [
    "winning_model = model\n",
    "winning_model.load_state_dict(torch.load(save_location_raw+winner+\".pth\"))\n",
    "winning_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84npSeHwk6N2"
   },
   "source": [
    "## Dataset & Data Loader Definition for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUKjZII9k3kH"
   },
   "outputs": [],
   "source": [
    "# Test Dataset definition\n",
    "class TestDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path, transform):\n",
    "        self.path = path\n",
    "        self.X = sorted([x.split('.')[0] for x in os.listdir(path)], key=int)\n",
    "        self.transform = transform  \n",
    "        \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, index):\n",
    "        x = self.transform(Image.open(self.path+self.X[index]+\".jpg\"))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eeEuJL9vJts"
   },
   "outputs": [],
   "source": [
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224,224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "test_dataset = TestDataset(\"test_data/\", test_transforms)\n",
    "test_dataloader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True) if torch.cuda.is_available()\\\n",
    "                else dict(shuffle=False, batch_size=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, **test_dataloader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OPrVqRjCvqJK"
   },
   "outputs": [],
   "source": [
    "def test_on_final_model(model, test_loader, criterion):\n",
    "  print('Final Testing...')\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    p = []\n",
    "    i = 0\n",
    "    for batch_idx, (data) in enumerate(test_loader):\n",
    "      data = data.to(device)\n",
    "      outputs = model(data)\n",
    "\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      p.append(predicted.cpu().numpy())# get predictions back to cpu. no more gpu\n",
    "\n",
    "    return p # This will be written to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfYEVikcvtXr",
    "outputId": "5759becc-7c35-4835-f504-dc75eef671fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Testing...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "p = test_on_final_model(winning_model, test_dataloader, criterion)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5NT6_x7vzsz"
   },
   "source": [
    "## Write submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_piaMTZyv3VO"
   },
   "outputs": [],
   "source": [
    "with open(save_location_raw+winner+\".csv\", 'w') as f:\n",
    "  f.write(f\"id,label\\n\")\n",
    "  for indx, pred in enumerate(np.concatenate(p)):\n",
    "    f.write(f\"{indx}.jpg,{idx_to_class[pred]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgkgZOjvwZk5"
   },
   "outputs": [],
   "source": [
    "print(winner)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Resnet18.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
